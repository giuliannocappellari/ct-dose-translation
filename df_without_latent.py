# -*- coding: utf-8 -*-
"""DF-Without Latent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZpuXylpGClKfPOq8h9piPZCRVPkjQxW
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pydicom
!pip install einops
!pip install diffusers
!pip install lpips
!pip install kornia

from __future__ import annotations

import argparse
import math
import os
import json
from pathlib import Path
from typing import Tuple
import pydicom
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from diffusers import DDPMScheduler
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from tqdm import tqdm
import lpips
from torch.utils.data import random_split
import kornia

def exists(x):
    return x is not None


def default(val, d):
    return d if val is None else val

def load_checkpoint(model: torch.nn.Module, ckpt_path: str | Path, map_location):
    ckpt = torch.load(ckpt_path, map_location=map_location)
    model.load_state_dict(ckpt)
    model.eval()
    return model

class SinusoidalPosEmb(nn.Module):
    """From "Improved DDPM"â€”encodes timestep t into a 1â€‘D embedding."""

    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        device = t.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = t[:, None] * emb[None, :]
        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)
        return emb

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

class Residual(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    def forward(self, x, *args, **kwargs):
        return self.fn(x, *args, **kwargs) + x

class ResnetBlockTime(nn.Module):
    """ResNet block with timestep conditioning (FiLMâ€‘style)."""

    def __init__(self, in_ch: int, out_ch: int, time_dim: int):
        super().__init__()
        self.mlp = nn.Sequential(Swish(), nn.Linear(time_dim, out_ch * 2))
        self.block1 = nn.Sequential(
            nn.GroupNorm(32, in_ch), Swish(), nn.Conv2d(in_ch, out_ch, 3, 1, 1)
        )
        self.block2 = nn.Sequential(
            nn.GroupNorm(32, out_ch), Swish(), nn.Conv2d(out_ch, out_ch, 3, 1, 1)
        )
        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):
        h = self.block1(x)
        scale, shift = self.mlp(t_emb).chunk(2, dim=1)
        h = h * (1 + scale[..., None, None]) + shift[..., None, None]
        h = self.block2(h)
        return h + self.res_conv(x)

class Downsample(nn.Module):
    def __init__(self, ch: int):
        super().__init__()
        self.conv = nn.Conv2d(ch, ch, 3, 2, 1)

    def forward(self, x):
        return self.conv(x)

class Upsample(nn.Module):
    def __init__(self, ch: int):
        super().__init__()
        self.conv = nn.Conv2d(ch, ch, 3, 1, 1)

    def forward(self, x):
        x = F.interpolate(x, scale_factor=2, mode="nearest")
        return self.conv(x)

class AttentionBlock(nn.Module):
    """Vanilla QKV selfâ€‘attention over HÃ—W positions."""

    def __init__(self, ch: int, num_heads: int = 4):
        super().__init__()
        self.norm = nn.GroupNorm(32, ch)
        self.qkv = nn.Conv2d(ch, ch * 3, 1)
        self.num_heads = num_heads
        self.proj = nn.Conv2d(ch, ch, 1)

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.norm(x)
        qkv = self.qkv(h).reshape(B, 3, self.num_heads, C // self.num_heads, H * W)
        q, k, v = qkv.unbind(dim=1)                 # each (B,H,dim,HW)
        q = q.permute(0, 1, 3, 2)                   # (B,H,HW,dim)
        k = k                                       # (B,H,dim,HW)
        attn = torch.softmax(                       # (B,H,HW,HW)
            torch.einsum("bhqd,bhdk->bhqk", q, k) / math.sqrt(C // self.num_heads),
            dim=-1)
        v = v.permute(0, 1, 3, 2)                   # (B,H,HW,dim)
        out = torch.einsum("bhqk,bhkd->bhqd", attn, v)   # (B,H,HW,dim)
        out = out.permute(0, 1, 3, 2).reshape(B, C, H, W)
        return x + self.proj(out)

class PixelDiffusionUNetConditional(nn.Module):
    """UNet that predicts Îµ given (x_t, cond) in **pixel space**."""

    def __init__(
        self,
        base_ch: int = 64,
        ch_mults: Tuple[int, ...] = (1, 2, 4, 8),
        time_dim: int = 256,
        attn_res: Tuple[int, ...] = (32, 64),  # selfâ€‘attn at 32Ã—32 and 64Ã—64
    ):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(time_dim),
            nn.Linear(time_dim, time_dim * 4),
            Swish(),
            nn.Linear(time_dim * 4, time_dim),
        )

        # init conv: concat noisy x and LD condition â†’ base_ch
        self.init_conv = nn.Conv2d(2, base_ch, 3, padding=1)

        # Downsampling path
        curr_ch = base_ch
        self.downs = nn.ModuleList()
        resolutions = []
        for i, mult in enumerate(ch_mults):
            out_ch = base_ch * mult
            block = nn.ModuleList(
                [
                    ResnetBlockTime(curr_ch, out_ch, time_dim),
                    ResnetBlockTime(out_ch, out_ch, time_dim),
                    AttentionBlock(out_ch) if 256 // (2 ** i) in attn_res else nn.Identity(),
                    Downsample(out_ch) if i < len(ch_mults) - 1 else nn.Identity(),
                ]
            )
            self.downs.append(block)
            resolutions.append(out_ch)
            curr_ch = out_ch

        # Bottleneck
        self.mid = nn.ModuleList(
            [
                ResnetBlockTime(curr_ch, curr_ch, time_dim),
                AttentionBlock(curr_ch),
                ResnetBlockTime(curr_ch, curr_ch, time_dim),
            ]
        )

        # Upsampling path (with skip connections)
        self.ups = nn.ModuleList()
        for i, mult in enumerate(reversed(ch_mults[:-1])):      # mult = 4,2,1
            skip_ch = base_ch * mult        # 256,128,64
            in_ch   = curr_ch + skip_ch     # 512+256, 256+128, 128+64
            out_ch  = skip_ch               # 256,128,64

            block = nn.ModuleDict({
                # ðŸ”´  always upsample; use curr_ch so conv sees correct inâ€‘channels
                "up":   Upsample(curr_ch),

                "res1": ResnetBlockTime(in_ch,  out_ch, time_dim),
                "res2": ResnetBlockTime(out_ch, out_ch, time_dim),
                "attn": AttentionBlock(out_ch)
                        if 256 // (2 ** (len(ch_mults) - 2 - i)) in attn_res
                        else nn.Identity(),
            })
            self.ups.append(block)
            curr_ch = out_ch                # 512â†’256â†’128â†’64

        self.final = nn.Sequential(
            nn.GroupNorm(32, curr_ch),
            Swish(),
            nn.Conv2d(curr_ch, 1, 3, padding=1),
        )

    def forward(self, x_t: torch.Tensor, t: torch.Tensor, cond: torch.Tensor):
      # 1) time embedding
      t_emb = self.time_mlp(t)

      # 2) concat LD condition + noisy ND
      h = self.init_conv(torch.cat([x_t, cond], dim=1))

      skips = []
      # ---- DOWN SAMPLING -------------------------------------------------
      for i, block in enumerate(self.downs):          # i = 0..len(downs)-1
          res1, res2, attn, down = block

          h = res1(h, t_emb)
          h = res2(h, t_emb)
          h = attn(h)

          # â–¸â–¸ save skip for all but the deepest level â—‚â—‚
          if i < len(self.downs) - 1:                 # â† only 0,1,2 for 4â€‘level UNet
              skips.append(h)

          h = down(h)                                 # downsample (noâ€‘op at bottom)

      # ---- BOTTLENECK ----------------------------------------------------
      for layer in self.mid:
          h = layer(h, t_emb) if isinstance(layer, ResnetBlockTime) else layer(h)

      # ---- UP SAMPLING ---------------------------------------------------
      for block in self.ups:
        h = block["up"](h)                     # 32â†’64â†’128â†’256
        h = torch.cat([h, skips.pop()], 1)     # spatial sizes now equal
        h = block["res1"](h, t_emb)
        h = block["res2"](h, t_emb)
        h = block["attn"](h)

      return self.final(h)

class CTPairsDataset(Dataset):
    """Loads paired lowâ€‘dose (LD) and normalâ€‘dose (ND) CT slices as PNG/TIFF."""

    def __init__(self, root: str | Path, transform=None):
        root = Path(root)

        # collect all your low/high folders however you like
        self.pairs = self.collect_pairs_by_position(root)
        self.transform = default(
            transform,
            transforms.Compose(
                [transforms.CenterCrop(256), transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]
            ),
        )

    def __len__(self):
        return len(self.pairs)

    def collect_pairs_by_position(self, root: str, sort: bool = True):
        """
        Walk quarter_3mm â†” full_3mm and quarter_1mm â†” full_1mm in parallel,
        and pair the i-th image in each patient directory by position.
        """
        root = Path(root)
        mapping = {
            "quarter_3mm": "full_3mm",
            "quarter_1mm": "full_1mm",
        }

        pairs = []
        for small_name, full_name in mapping.items():
            small_root = root / small_name
            full_root  = root / full_name

            # each subfolder under small_root is a patient ID
            for patient_dir in sorted(small_root.iterdir()):
                if not patient_dir.is_dir():
                    continue

                # match the same patient under the full folder
                full_patient_dir = full_root / patient_dir.name
                if not full_patient_dir.exists():
                    continue

                # grab all images anywhere under that patient (rglob)
                small_imgs = list(patient_dir.rglob("*.IMA"))
                full_imgs  = list(full_patient_dir.rglob("*.IMA"))

                if sort:
                    small_imgs.sort()
                    full_imgs.sort()

                # pair by index
                for small_img, full_img in zip(small_imgs, full_imgs):
                    pairs.append((small_img, full_img))

        return pairs

    def __getitem__(self, idx):
        ld_path, nd_path = self.pairs[idx]
        ds_q = pydicom.dcmread(ld_path)
        ds_f = pydicom.dcmread(nd_path)
        img_q = ds_q.pixel_array.astype(np.float32)
        img_f = ds_f.pixel_array.astype(np.float32)

        # Normalize pixel values to [0, 1]
        img_q = (img_q - img_q.min()) / (img_q.max() - img_q.min() + 1e-5)
        img_f = (img_f - img_f.min()) / (img_f.max() - img_f.min() + 1e-5)

        # Convert to 8-bit and PIL Image
        img_q = (img_q * 255).astype(np.uint8)
        img_q = Image.fromarray(img_q)
        # Convert to 8-bit and PIL Image
        img_f = (img_f * 255).astype(np.uint8)
        img_f = Image.fromarray(img_f)

        ld, nd = self.transform(img_q), self.transform(img_f)
        return ld, nd

def forward_diffusion_sample(x0: torch.Tensor, t: torch.Tensor, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod):
    """Returns noisy x_t, plus the noise (Îµ) used to generate it."""
    noise = torch.randn_like(x0)
    sqrt_ac = sqrt_alphas_cumprod[t][:, None, None, None]
    sqrt_om = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]
    return sqrt_ac * x0 + sqrt_om * noise, noise

@torch.no_grad()
def sample(model, ld_batch, scheduler, guidance=5.0):
    """DDIMâ€‘like sampling with classifierâ€‘free guidance."""
    model.eval()
    device = ld_batch.device
    B = ld_batch.size(0)
    x = torch.randn_like(ld_batch)
    for i, t in enumerate(reversed(scheduler.timesteps)):
        t_batch = torch.full((B,), t, device=device, dtype=torch.long)
        eps_cond = model(x, t_batch, ld_batch)
        eps_uncond = model(x, t_batch, torch.zeros_like(ld_batch))
        eps = eps_uncond + guidance * (eps_cond - eps_uncond)
        out = scheduler.step(eps, t_batch[0], x)
        x = out.prev_sample
    return x.clamp(-1, 1)

def get_transform():
    return transforms.Compose(
        [
            transforms.CenterCrop(256),
            transforms.ToTensor(),  # â†’ [0,1]
            transforms.Normalize(0.5, 0.5),  # â†’ [â€‘1,1]
        ]
    )

def load_and_preprocess(dicom_path: str | Path, tfm) -> torch.Tensor:
    """Returns LD slice as **[1,1,256,256]** in [â€‘1,1]."""
    ds = pydicom.dcmread(str(dicom_path))
    arr = ds.pixel_array.astype(np.float32)
    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-5)
    pil = Image.fromarray((arr * 255).astype("uint8"))
    return tfm(pil).unsqueeze(0)  # [1,1,H,W]

@torch.no_grad()
def sample_ddim_guided(ld_img: torch.Tensor, scheduler, diffusion, guidance_scale: float = 5.0):
    """Generate a ND prediction from a batch of LD images (pixelâ€‘space)."""

    device = ld_img.device
    B = ld_img.size(0)
    x = torch.randn_like(ld_img)  # start from pure noise

    for t in scheduler.timesteps:
        t_int = int(t.item() if isinstance(t, torch.Tensor) else t)
        t_batch = torch.full((B,), t_int, device=device, dtype=torch.long)

        # unconditional (no LD condition)
        eps_uncond = diffusion(x, t_batch, cond=torch.zeros_like(ld_img))
        # conditional
        eps_cond = diffusion(x, t_batch, cond=ld_img)
        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)

        out = scheduler.step(eps, t_int, x, return_dict=True)
        x = out.prev_sample
    return x  # still in [-1,1]

@torch.no_grad()
def predict(ld_img: torch.Tensor, model, scheduler):
    """Runs diffusion and returns prediction in **[0,1]** as [1,1,H,W]."""
    pred = sample_ddim_guided(ld_img, scheduler, model)
    return (pred + 1) / 2  # to [0,1]

# simple CT loader (identical to training)
_transform = transforms.Compose(
    [transforms.CenterCrop(256), transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]
)

def evaluate(model, val_paths, scheduler, device, tfm):
    ssim_mod = kornia.metrics.SSIM(window_size=11, max_val=1.0, padding="same").to(device)
    scores = []
    scores_2 = []
    pbar = tqdm(val_paths, desc="val", unit="img")
    for ld_path in pbar:
        ld_t = load_and_preprocess(ld_path, tfm).to(device)
        with torch.no_grad():
            pred_t = sample_ddim_guided(ld_t, scheduler, model)  # [-1,1]
            pred_t = (pred_t + 1) / 2                           # [0,1]
        gt_t = load_and_preprocess(ld_path, tfm).to(device)                      # [1,1,H,W]
        gt   = (gt_t * 0.5 + 0.5).clamp(0,1)                                  # [1,1,H,W]
        # 4) compute SSIM map & reduce
        with torch.no_grad():
            ssim_map   = ssim_mod(pred_t, gt)                             # [1,1,H,W]
            ssim_score = ssim_map.mean().item()                             # scalar
        scores.append({"path": ld_path, "ssim": ssim_score})
        scores_2.append(ssim_score)
        pbar.set_postfix(ssim=f"{ssim_score:.4f}")
        json.dump(scores, open("scores.json", "w"))
    return sum(scores_2) / len(scores_2)

def train_with_eval(
    model,
    train_loader,
    val_paths,
    scheduler,
    optimizer,
    sqrt_ac,
    sqrt_om,
    device,
    drop_prob=0.1,
    epochs=10,
):
    import torch.nn.functional as F

    def forward_diff(x0, t):
        noise = torch.randn_like(x0)
        x = sqrt_ac[t][:, None, None, None] * x0 + sqrt_om[t][:, None, None, None] * noise
        return x, noise

    best_ssim = 0.0
    for ep in range(1, epochs + 1):
        print(ep)
        model.train()
        running = 0.0
        pbar = tqdm(train_loader, desc=f"train {ep}")
        for ld, nd in pbar:
            ld, nd = ld.to(device), nd.to(device)
            B = nd.size(0)
            t = torch.randint(0, scheduler.num_train_timesteps, (B,), device=device, dtype=torch.long)
            x_t, noise = forward_diff(nd, t)
            mask = (torch.rand(B, device=device) < drop_prob).view(B, 1, 1, 1)
            cond = torch.where(mask, torch.zeros_like(ld), ld)
            pred = model(x_t, t, cond)
            loss = F.mse_loss(pred, noise)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running += loss.item() * B
            pbar.set_postfix(loss=f"{loss.item():.4f}")
        train_loss = running / len(train_loader.dataset)

        # validation SSIM
        model.eval()
        avg_ssim = evaluate(model, val_paths, scheduler, device, get_transform())
        print(f"Epoch {ep}: train loss {train_loss:.4f} | val SSIM {avg_ssim:.4f}")
        torch.save(model.state_dict(), f"/content/drive/MyDrive/CT Models/DiffusionNonLatent/diffusion_epoch_{ep}.pth")
    best_ssim = max(best_ssim, avg_ssim)
    print(f"Best SSIM across epochs: {best_ssim:.4f}")

scheduler = DDPMScheduler(
    num_train_timesteps=1000,
    beta_start=1e-4,
    beta_end=0.02,
    beta_schedule="squaredcos_cap_v2",  # or the schedule of your choice
)
betas   = scheduler.betas              # [T]  tensor
device  = torch.device("cuda")
betas   = betas.to(device)

# 2) derive Î±, cumulative  Î±Ì„, and the two squareâ€‘roots
alphas            = 1.0 - betas                    # Î±_t
alphas_cumprod    = torch.cumprod(alphas, dim=0)   # Î±Ì„_t
sqrt_ac           = torch.sqrt(alphas_cumprod)     # âˆšÎ±Ì„_t   (your âˆšac)
sqrt_om           = torch.sqrt(1.0 - alphas_cumprod)  # âˆš(1âˆ’Î±Ì„_t) (your âˆšom)

# load model
model = PixelDiffusionUNetConditional().to(device)
optimizer = torch.optim.AdamW(
    params      = model.parameters(),
    lr          = 1e-4,          # good starting point for 256Â² CT slices
    betas       = (0.9, 0.95),   # slightly lower Î²â‚‚ improves noise prediction
    weight_decay= 1e-2           # mild L2 regularisation on weights
)

ds = CTPairsDataset("/content/drive/MyDrive/CT/", transform=get_transform())
n_val   = int(len(ds) * 0.1)
n_train = len(ds) - n_val

train_ds, vaal_ds = random_split(ds, [n_train, n_val])

train_loader = DataLoader(train_ds, batch_size=12, shuffle=True,  num_workers=4)

val_paths = json.load(open("/content/drive/MyDrive/CT/image_paths.json"))
len(val_paths[0:100])

train_with_eval(model, train_loader, val_paths[0:100], scheduler, optimizer, sqrt_ac, sqrt_om, device, epochs=140)

